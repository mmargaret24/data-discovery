{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a2584dd",
   "metadata": {},
   "source": [
    "# Using Sherlock out-of-the-box\n",
    "This notebook is adopted from Sherlock's guide on how to predict a semantic type for a given table column.\n",
    "\n",
    "Key tasks performed in this notebook are (Task 1):\n",
    "- read data files and filter for CSV format\n",
    "- using Sherlock to predict semantics of the columns with varying confidence thresholds\n",
    "- export predicted semantics into the output folder as CSV files\n",
    "\n",
    "Pre-requisite:\n",
    "- Execute in Python 3.8.0 environment with Sherlock installed \n",
    "- Data files (CSV) are ready to be imported\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665a340",
   "metadata": {},
   "source": [
    "![Workflow for the experimental setup in leveraging column semantics for data discovery.](image/workflowv2.png \"Workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc06ce",
   "metadata": {},
   "source": [
    "## 1. Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for folder containing data files, log files and output files\n",
    "DIR_DATASET = '/ivi/inde/mmargaret/data_search_e_data_csv/' # replace with folder containing NTCIR's CSV datafiles\n",
    "DIR_LOG = '/ivi/inde/mmargaret/sherlock-project/log_2/' # replace with folder to store log files\n",
    "DIR_OUTPUT = '/ivi/inde/mmargaret/sherlock-project/output_2/' # replace with folder to output enriched datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c929f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY LOCAL DIR\n",
    "# DIR_DATASET = '/Users/mmargaret/Documents/[UVA] Thesis/sherlock-project/data/data_search_e_data_csv/'\n",
    "# DIR_LOG = '/Users/mmargaret/Documents/[UVA] Thesis/sherlock-project/log_2/'\n",
    "# DIR_OUTPUT = '/Users/mmargaret/Documents/[UVA] Thesis/sherlock-project/output_2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8855b7c",
   "metadata": {},
   "source": [
    "### Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce71eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='{}{}'.format(DIR_LOG,datetime.now().strftime('%Y%m%d_%H%M_sherlock.log')), mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s : %(message)s', datefmt='%m/%d/%Y %I:%M')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.info('- LOGGING STARTS -')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6bdd41",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import os\n",
    "import sys\n",
    "    \n",
    "from sherlock import helpers\n",
    "from sherlock.deploy.model import SherlockModel\n",
    "from sherlock.functional import extract_features_to_csv\n",
    "from sherlock.features.paragraph_vectors import initialise_pretrained_model, initialise_nltk\n",
    "from sherlock.features.preprocessing import (\n",
    "    extract_features,\n",
    "    convert_string_lists_to_lists,\n",
    "    prepare_feature_extraction,\n",
    "    load_parquet_values,\n",
    ")\n",
    "from sherlock.features.word_embeddings import initialise_word_embeddings\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1101303",
   "metadata": {},
   "source": [
    "### Initialize Sherlock's feature extraction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8682ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature extraction by downloading 4 files:\n",
      "        \n",
      " ../sherlock/features/glove.6B.50d.txt, \n",
      " ../sherlock/features/par_vec_trained_400.pkl.docvecs.vectors_docs.npy,\n",
      "        \n",
      " ../sherlock/features/par_vec_trained_400.pkl.trainables.syn1neg.npy, and \n",
      " ../sherlock/features/par_vec_trained_400.pkl.wv.vectors.npy.\n",
      "        \n",
      "All files for extracting word and paragraph embeddings are present.\n",
      "Initialising word embeddings\n",
      "Initialise Word Embeddings process took 0:00:04.636650 seconds.\n",
      "Initialise Doc2Vec Model, 400 dim, process took 0:00:02.604305 seconds. (filename = ../sherlock/features/par_vec_trained_400.pkl)\n",
      "Initialised NLTK, process took 0:00:00.149001 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mmargaret/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mmargaret/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "prepare_feature_extraction()\n",
    "initialise_word_embeddings()\n",
    "initialise_pretrained_model(400)\n",
    "initialise_nltk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f30d7",
   "metadata": {},
   "source": [
    "### Get the list of all data files for processing\n",
    "Retrieve all CSV filenames in the specified folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ba26aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = os.listdir(DIR_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bacb96ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Files: 2919\n"
     ]
    }
   ],
   "source": [
    "file_list = [id for id in _ if '.csv' in id]\n",
    "logging.info('Number of Files: {}'.format(len(file_list)))\n",
    "print('Number of Files: {}'.format(len(file_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6095f7",
   "metadata": {},
   "source": [
    "## 2. Define Utilities Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1736dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictedLabels(y_pred_proba, classes, threshold=0.0):\n",
    "    \"\"\"\n",
    "        This function retrieve the predicted semantics by assigning classes with highest probability \n",
    "        that is at least the same or higher than the parameter: \"thresholds\". \n",
    "        Input: \n",
    "            y_pred_proba: \n",
    "            classes:\n",
    "            threshold:\n",
    "        Output: returns a list of predicted semantics \n",
    "    \"\"\"\n",
    "    pred_scores = np.max(y_pred_proba, axis=1)\n",
    "    index_threshold = np.where(pred_scores >= threshold)[0]\n",
    "    y_pred_int = np.argmax(y_pred_proba, axis=1)[index_threshold]\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = classes\n",
    "\n",
    "    return encoder.inverse_transform(y_pred_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac876f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractIDSemanticsWithColumnNames(filename):\n",
    "    \"\"\"\n",
    "        This function:\n",
    "        (1) read the dataset given by the \"filename\", \n",
    "        (2) using Sherlock to: extract their features, initialise Sherlock model and predict their semantics. \n",
    "        (3) extract other features, such as column names and column types\n",
    "        Input: \n",
    "            filename: \n",
    "        Output: returns a dictionary containing semantics and features of one data file\n",
    "    \"\"\"\n",
    "    \n",
    "    IDSemanticsColumns = {'data_filename':filename, 'colSemantics': [], 'colNames':[]}\n",
    "    try:\n",
    "        # read files\n",
    "        with open(DIR_DATASET + filename, errors='ignore') as f:\n",
    "            a_doc = pd.read_csv(f)\n",
    "        \n",
    "        # column stats\n",
    "        col_types = a_doc.dtypes.tolist()\n",
    "        col_complete = pct_completeness(a_doc)\n",
    "        \n",
    "        a_doc = a_doc.select_dtypes(include=[object]).astype(str)\n",
    "        col_len = avgColumnLength(a_doc)\n",
    "        data = pd.Series(a_doc.transpose().values.tolist(), name=\"values\") #format it to list of values by columns\n",
    "\n",
    "        # sherlock extract features\n",
    "        extract_features(\"../temporary.csv\",data)\n",
    "        feature_vectors = pd.read_csv(\"../temporary.csv\", dtype=np.float32)\n",
    "\n",
    "        # sherlock init and predict with pre-trained model\n",
    "        model = SherlockModel();\n",
    "        model.initialize_model_from_json(with_weights=True, model_id=model_id);\n",
    "        \n",
    "        # PREDICT\n",
    "        predicted_proba = model.predict_proba(feature_vectors, model_id)\n",
    "        predicted_scores = np.max(predicted_proba, axis=1).round(4)\n",
    "\n",
    "        # return dictionary with id: id of the doc, list of the columns' semantics, list of the columns' names\n",
    "        IDSemanticsColumns = {'data_filename':filename\n",
    "                              , 'colSemantics': list(getPredictedLabels(predicted_proba, classes))\n",
    "                              , 'colSemantics_s10': list(getPredictedLabels(predicted_proba, classes, 0.1))\n",
    "                              , 'colSemantics_s20': list(getPredictedLabels(predicted_proba, classes, 0.2))\n",
    "                              , 'colSemantics_s30': list(getPredictedLabels(predicted_proba, classes, 0.3))\n",
    "                              , 'colSemantics_s40': list(getPredictedLabels(predicted_proba, classes, 0.4))\n",
    "                              , 'colSemantics_s50': list(getPredictedLabels(predicted_proba, classes, 0.5))\n",
    "                              , 'colSemantics_s60': list(getPredictedLabels(predicted_proba, classes, 0.6))\n",
    "                              , 'colSemantics_s70': list(getPredictedLabels(predicted_proba, classes, 0.7))\n",
    "                              , 'colSemantics_s80': list(getPredictedLabels(predicted_proba, classes, 0.8))\n",
    "                              , 'colSemantics_s90': list(getPredictedLabels(predicted_proba, classes, 0.9))\n",
    "                              , 'colSemantics_s95': list(getPredictedLabels(predicted_proba, classes, 0.95))\n",
    "                              , 'colSemantics_s98': list(getPredictedLabels(predicted_proba, classes, 0.98))\n",
    "                              , 'colSemantics_s99': list(getPredictedLabels(predicted_proba, classes, 0.99))\n",
    "                              , 'colNames':list(a_doc.columns)\n",
    "                              , 'colTypes': col_types\n",
    "                              , 'colLen': col_len\n",
    "                              , 'colComplete': col_complete\n",
    "                              , 'colScores': list(predicted_scores)}\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error('Unable to extract: {}'.format(filename))\n",
    "        print('Unable to extract: {}'.format(filename))\n",
    "        \n",
    "        print(e)\n",
    "        logging.error(e, exc_info=True)\n",
    "        \n",
    "        global error_list\n",
    "        error_list += [filename]\n",
    "        \n",
    "    return IDSemanticsColumns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd5b3d",
   "metadata": {},
   "source": [
    "### Test Extraction of Semantics\n",
    "Test for extraction of one sample of data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60116cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "error_list=[] # initialise the list to store filenames with prediction errors\n",
    "model_id = \"sherlock\"\n",
    "classes = np.load(f\"../model_files/classes_{model_id}.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bccb8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Extracting Features:   0%|                               | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30582267f36c39a6ff33b0e38787eaa72f9ad84192498830816d3d2bf5b2e73b.text.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Extracting Features:   8%|█▉                     | 1/12 [00:00<00:01,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 12/12 [00:00<00:00, 18.56it/s]\n",
      "2022-06-02 21:47:18.710884: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-06-02 21:47:18.724400: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc343598620 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-02 21:47:18.724426: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_filename': '30582267f36c39a6ff33b0e38787eaa72f9ad84192498830816d3d2bf5b2e73b.text.csv', 'colSemantics': ['area', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state'], 'colSemantics_s10': ['area', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state'], 'colSemantics_s20': ['area', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state'], 'colSemantics_s30': ['area', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state'], 'colSemantics_s40': ['area', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state'], 'colSemantics_s50': ['area', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state'], 'colSemantics_s60': ['area', 'state', 'state', 'state', 'state', 'state', 'state', 'state', 'state'], 'colSemantics_s70': ['state', 'state', 'state', 'state'], 'colSemantics_s80': ['state', 'state', 'state', 'state'], 'colSemantics_s90': [], 'colSemantics_s95': [], 'colSemantics_s98': [], 'colSemantics_s99': [], 'colNames': ['Reporting Area', 'Vibriosis (Any species of the family Vibrionaceae, other than toxigenic Vibrio cholerae O1 or O139), Confirmed Current week, flag ', 'Vibriosis (Any species of the family Vibrionaceae, other than toxigenic Vibrio cholerae O1 or O139), Confirmed Previous 52 weeks Med, flag', 'Vibriosis (Any species of the family Vibrionaceae, other than toxigenic Vibrio cholerae O1 or O139), Confirmed Previous 52 weeks Max, flag', 'Vibriosis (Any species of the family Vibrionaceae, other than toxigenic Vibrio cholerae O1 or O139), Confirmed Cum 2018, flag', 'Vibriosis (Any species of the family Vibrionaceae, other than toxigenic Vibrio cholerae O1 or O139), Confirmed Cum 2017, flag', 'Vibriosis (Any species of the family Vibrionaceae, other than toxigenic Vibrio cholerae O1 or O139), Probable Current week, flag', 'Vibriosis (Any species of the family Vibrionaceae, other than toxigenic Vibrio cholerae O1 or O139), Probable Previous 52 weeks Med, flag', 'Vibriosis (Any species of the family Vibrionaceae, other than toxigenic Vibrio cholerae O1 or O139), Probable Previous 52 weeks Max, flag', 'Vibriosis (Any species of the family Vibrionaceae, other than toxigenic Vibrio cholerae O1 or O139), Probable Cum 2018, flag', 'Vibriosis (Any species of the family Vibrionaceae, other than toxigenic Vibrio cholerae O1 or O139), Probable Cum 2017, flag', 'Location 1'], 'colTypes': [dtype('O'), dtype('int64'), dtype('int64'), dtype('float64'), dtype('O'), dtype('float64'), dtype('O'), dtype('float64'), dtype('O'), dtype('float64'), dtype('O'), dtype('float64'), dtype('O'), dtype('float64'), dtype('O'), dtype('float64'), dtype('O'), dtype('float64'), dtype('O'), dtype('float64'), dtype('O'), dtype('float64'), dtype('O'), dtype('O'), dtype('float64')], 'colLen': [9.0149, 1.2761, 2.9558, 2.9558, 2.287, 2.4104, 1.2216, 2.9558, 2.9558, 2.2084, 2.1642, 27.6122], 'colComplete': [1.0, 1.0, 1.0, 0.1381, 0.8619, 0.9779, 0.0221, 0.9779, 0.0221, 0.6435, 0.3565, 0.7052, 0.2948, 0.1108, 0.8892, 0.9779, 0.0221, 0.9779, 0.0221, 0.6042, 0.3958, 0.5821, 0.4179, 0.8209, 0.0], 'colScores': [0.6099, 0.4955, 0.8468, 0.8468, 0.637, 0.6508, 0.5418, 0.8468, 0.8468, 0.6757, 0.5187, 0.6517]}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# TEST function\n",
    "logging.info('- TEST START -')\n",
    "\n",
    "test_file = file_list[1]\n",
    "print(test_file)\n",
    "logging.info('filename: {}'.format(test_file))\n",
    "\n",
    "test_extract = extractIDSemanticsWithColumnNames(test_file)\n",
    "print (test_extract)\n",
    "logging.info('extraction: {}'.format(test_extract))\n",
    "\n",
    "print (error_list)\n",
    "logging.info('error list: {}'.format(error_list))\n",
    "\n",
    "logging.info('- TEST END -')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0b1b6",
   "metadata": {},
   "source": [
    "## 3. Semantics Extraction\n",
    "begin to predict semantics for all data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3001d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = os.listdir(DIR_OUTPUT) # retrieve filenamne\n",
    "enrich_list = [] # initiliase the list to store the output of \n",
    "output_filenames = [] # initialise the list to store filenames that have been processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec9ae30",
   "metadata": {},
   "source": [
    "### Get the list of latest extracted semantics\n",
    "If you are running a huge volume of files and require to continue from the latest run, use it. \n",
    "If you want to run a fresh run each time, skip this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "defa3fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Output Files: 0\n",
      "Unable to retrieve latest output\n",
      "max() arg is an empty sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    output_list = [DIR_OUTPUT + str(id) for id in _ if 'enriched_part_' in id]\n",
    "    logging.info('Number of Output Files: {}'.format(len(output_list)))\n",
    "    print('Number of Output Files: {}'.format(len(output_list)))\n",
    "\n",
    "    latest_output = max(output_list, key=os.path.getctime)\n",
    "    logging.info('Latest Output Filename: {}'.format(latest_output))\n",
    "    print('Latest Output Filename: {}'.format(latest_output))\n",
    "\n",
    "    output_df = pd.read_csv(latest_output)\n",
    "    output_filenames = output_df['data_filename'].tolist()\n",
    "    enrich_list = output_df.to_dict('records')\n",
    "    \n",
    "    logging.info('Number of Extracted Dataset: {}'.format(len(output_filenames)))\n",
    "    print('Number of Extracted Dataset: {}'.format(len(output_filenames)))\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error('Unable to retrieve latest output')\n",
    "    print('Unable to retrieve latest output')\n",
    "    \n",
    "    logging.error(e, exc_info=True)\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "output_filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12701567",
   "metadata": {},
   "source": [
    "### Start Extraction \n",
    "Predict semantics for all data files in the specified folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ce676",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('- EXTRACT START -')\n",
    "\n",
    "error_list=[] # reset the list to keep track of filenames with prediction that were not successful\n",
    "model_id = \"sherlock\"\n",
    "classes = np.load(f\"../model_files/classes_{model_id}.npy\", allow_pickle=True)\n",
    "col_csv = ['data_filename', 'colSemantics'\n",
    "           , 'colSemantics_s10', 'colSemantics_s20', 'colSemantics_s30', 'colSemantics_s40', 'colSemantics_s50'\n",
    "           , 'colSemantics_s60', 'colSemantics_s70', 'colSemantics_s80', 'colSemantics_s90', 'colSemantics_s95'\n",
    "           , 'colSemantics_s98', 'colSemantics_s99', 'colNames', 'colScores', 'colComplete', 'colTypes', 'colLen']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22ff0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  60%|██████████████▍         | 3/5 [00:00<00:00, 20.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 5/5 [00:00<00:00, 20.20it/s]\n",
      "Extracting Features:   8%|█▉                     | 1/12 [00:00<00:01,  8.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- i: 0 -\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 12/12 [00:00<00:00, 19.39it/s]\n",
      "Extracting Features: 100%|████████████████████████| 1/1 [00:00<00:00, 59.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmargaret/opt/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3377: DtypeWarning: Columns (8,9,10,11,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Extracting Features:  25%|█████▊                 | 3/12 [00:00<00:00, 10.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 12/12 [00:01<00:00, 11.78it/s]\n",
      "Extracting Features: 100%|███████████████████████| 4/4 [00:00<00:00, 344.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmargaret/opt/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  \"\"\"\n",
      "Extracting Features:  22%|█████▎                  | 2/9 [00:00<00:01,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 9/9 [00:02<00:00,  3.74it/s]\n",
      "Extracting Features: 100%|███████████████████████| 6/6 [00:00<00:00, 147.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  33%|███████▋               | 4/12 [00:00<00:01,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 12/12 [00:00<00:00, 15.51it/s]\n",
      "Extracting Features: 100%|███████████████████████| 2/2 [00:00<00:00, 171.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|███████████████████████| 7/7 [00:00<00:00, 231.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|█████████████████████| 33/33 [00:00<00:00, 191.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Extracting Features:   0%|                                | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- i: 10 -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Extracting Features:  11%|██▋                     | 1/9 [00:00<00:02,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 9/9 [00:00<00:00,  9.23it/s]\n",
      "Extracting Features:  22%|█████▎                  | 2/9 [00:00<00:00,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 9/9 [00:01<00:00,  5.74it/s]\n",
      "Extracting Features: 100%|████████████████████████| 3/3 [00:04<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Features: 100%|███████████████████████| 1/1 [00:00<00:00, 211.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  33%|████████                | 1/3 [00:00<00:00,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 3/3 [00:00<00:00,  6.60it/s]\n",
      "Extracting Features:  22%|█████                  | 4/18 [00:00<00:01, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 18/18 [00:00<00:00, 21.44it/s]\n",
      "Extracting Features: 100%|████████████████████████| 1/1 [00:00<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  15%|███▌                   | 4/26 [00:00<00:00, 25.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 26/26 [00:03<00:00,  8.37it/s]\n",
      "Extracting Features: 100%|███████████████████████| 2/2 [00:00<00:00, 133.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 3/3 [00:00<00:00, 78.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|███████████████████████| 2/2 [00:00<00:00, 139.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- i: 20 -\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Features: 100%|████████████████████████| 2/2 [00:01<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Features: 100%|███████████████████████| 8/8 [00:00<00:00, 236.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|███████████████████████| 7/7 [00:00<00:00, 197.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 3/3 [00:00<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Features:   0%|                                | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 2/2 [00:00<00:00,  5.79it/s]\n",
      "Extracting Features:  57%|█████████████▋          | 4/7 [00:00<00:00, 18.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 7/7 [00:00<00:00, 19.23it/s]\n",
      "Extracting Features: 100%|███████████████████████| 1/1 [00:00<00:00, 251.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Extracting Features:   0%|                                | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 9/9 [00:00<00:00, 14.02it/s]\n",
      "Extracting Features: 100%|███████████████████████| 2/2 [00:00<00:00, 164.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 1/1 [00:00<00:00, 12.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- i: 30 -\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Features: 100%|████████████████████████| 4/4 [00:00<00:00, 25.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  33%|████████                | 2/6 [00:00<00:00, 13.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 6/6 [00:00<00:00, 18.07it/s]\n",
      "Extracting Features: 100%|████████████████████████| 1/1 [00:00<00:00, 74.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  14%|███▏                   | 3/22 [00:00<00:01, 18.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 22/22 [00:02<00:00,  7.91it/s]\n",
      "Extracting Features:  33%|███████▋               | 4/12 [00:00<00:00, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 12/12 [00:00<00:00, 20.62it/s]\n",
      "Extracting Features:  33%|███████▋               | 5/15 [00:00<00:00, 10.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 15/15 [00:00<00:00, 22.40it/s]\n",
      "Extracting Features: 0it [00:00, ?it/s]\n",
      "Extracting Features:   0%|                               | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to extract: 046c3561b610ab29de866f59abb248ecf1a32fc190647c5d456784bd06d36018.text.csv\n",
      "No columns to parse from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Extracting Features:   9%|██                     | 1/11 [00:00<00:02,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 11/11 [00:02<00:00,  4.08it/s]\n",
      "Extracting Features:   0%|                               | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 44/44 [00:00<00:00, 90.94it/s]\n",
      "Extracting Features: 100%|███████████████████████| 6/6 [00:00<00:00, 115.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- i: 40 -\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Features: 100%|████████████████████████| 5/5 [00:00<00:00, 53.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|███████████████████████| 2/2 [00:00<00:00, 281.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|███████████████████████| 1/1 [00:00<00:00, 240.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmargaret/opt/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3377: DtypeWarning: Columns (1,2,5,6,7,8,10,13,17,18,19,21,22,23,24,25,26,27,28,33,34,35,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Extracting Features:   8%|█▊                     | 3/37 [00:00<00:01, 24.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 37/37 [00:02<00:00, 15.56it/s]\n",
      "Extracting Features:  33%|███████▋               | 5/15 [00:00<00:00, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 15/15 [00:00<00:00, 22.96it/s]\n",
      "Extracting Features:   6%|█▍                     | 2/32 [00:00<00:02, 13.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 32/32 [00:03<00:00,  9.41it/s]\n",
      "Extracting Features: 100%|███████████████████████| 2/2 [00:00<00:00, 183.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  20%|████▊                   | 1/5 [00:00<00:00,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 5/5 [00:00<00:00,  9.43it/s]\n",
      "Extracting Features:  33%|████████                | 2/6 [00:00<00:00,  9.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 6/6 [00:00<00:00,  9.49it/s]\n",
      "Extracting Features: 100%|███████████████████████| 2/2 [00:00<00:00, 102.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- i: 50 -\n",
      "Unable to extract: 131996a5027d1809789f1b20749c40d158883a4d4f242e568fbc1aa9ae2deffe.text.csv\n",
      "Error tokenizing data. C error: Expected 1 fields in line 9, saw 8\n",
      "\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Features:  21%|████▉                  | 3/14 [00:00<00:00, 22.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 14/14 [00:00<00:00, 25.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4j/b74xw8zn3_9dq5pwdxr7vtlr0000gn/T/ipykernel_8915/1995255503.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0menrich_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mextractIDSemanticsWithColumnNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4j/b74xw8zn3_9dq5pwdxr7vtlr0000gn/T/ipykernel_8915/3076002771.py\u001b[0m in \u001b[0;36mextractIDSemanticsWithColumnNames\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# PREDICT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#         predicted_labels = model.predict(feature_vectors, model_id)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mpredicted_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mpredicted_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/[UVA] Thesis/sherlock-project/sherlock/deploy/model.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, model_id)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_cols_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_cols_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"par\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_cols_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rest\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             ]\n\u001b[1;32m    148\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;31m# Setup work for each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_training_eval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;31m# Reset metrics on all the distributed (cloned) models.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3257\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3258\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3259\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each data file: \n",
    "- if datafile had been previously processed, skip the file. \n",
    "- otherwise, extract semantics prediction, column names, etc; store them in a list.\n",
    "- for every 10 data files processed, store a temporary file containing latest collection of prediction and error.\n",
    "\"\"\"\n",
    "\n",
    "for i in range(0, len(file_list)):\n",
    "    \n",
    "    # so that it does not need to rerun existing output\n",
    "    if (file_list[i] in output_filenames):\n",
    "        logging.info('Existed: {} skipped'.format(file_list[i]))\n",
    "        print('Existed: {} skipped'.format(file_list[i]))\n",
    "        continue\n",
    "        \n",
    "    enrich_list += [extractIDSemanticsWithColumnNames(file_list[i])]\n",
    "    if i%10==0:\n",
    "        \n",
    "        logging.info('i: {}'.format(i))\n",
    "        sys.stdout.write('- i: {} -'.format(i))\n",
    "        sys.stdout.write('\\n')\n",
    "        \n",
    "        pd.DataFrame(enrich_list\n",
    "                     , columns = col_csv).to_csv(DIR_OUTPUT +'enriched_part_' + str(i) +'.csv'\n",
    "                     , index=False)\n",
    "        \n",
    "        pd.DataFrame(error_list\n",
    "             , columns=['data_filename']).to_csv(DIR_OUTPUT + 'error_part_' + str(i) +'.csv'\n",
    "             , index=False)\n",
    "        \n",
    "logging.info('- EXTRACT END -')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f65cf9c",
   "metadata": {},
   "source": [
    "## 4. Export \n",
    "Export extracted semantics and list of files with errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a1153",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(enrich_list\n",
    "             , columns = col_csv).to_csv(DIR_OUTPUT +'enriched_all.csv'\n",
    "             , index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00760329",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(error_list\n",
    "             , columns=['data_filename']).to_csv(DIR_OUTPUT + 'error_all.csv'\n",
    "             , index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fdfcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('- EOF -')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
