{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a2584dd",
   "metadata": {},
   "source": [
    "# Using Sherlock out-of-the-box\n",
    "This notebook is adopted from Sherlock's guide on how to predict a semantic type for a given table column.\n",
    "\n",
    "Key tasks performed in this notebook are (Task 1):\n",
    "- read data files and filter for CSV format\n",
    "- using Sherlock to predict semantics of the columns with varying confidence thresholds\n",
    "- export predicted semantics into the output folder as CSV files\n",
    "\n",
    "Pre-requisite:\n",
    "- Data files (CSV) are ready to be imported\n",
    "- Notebook was executed inside Sherlock \"notebooks\" folder installed in Python 3.7.0 environment \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10519d8",
   "metadata": {},
   "source": [
    "![Workflow for the experimental setup in leveraging column semantics for data discovery.](image/workflowv2.png \"Workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc06ce",
   "metadata": {},
   "source": [
    "## 1. Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d60b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path for folder containing data files, log files and output files\n",
    "# DIR_DATASET = '/ivi/inde/mmargaret/data_search_e_data_csv/' # replace with folder containing NTCIR's CSV datafiles\n",
    "# DIR_LOG = '/ivi/inde/mmargaret/sherlock-project/log_2/' # replace with folder to store log files\n",
    "# DIR_OUTPUT = '/ivi/inde/mmargaret/sherlock-project/output_2/' # replace with folder to output enriched datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c929f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY LOCAL DIR\n",
    "DIR_DATASET = '/Users/mmargaret/Documents/[UVA] Thesis/sherlock-project/data/data_search_e_data_csv/'\n",
    "DIR_LOG = '/Users/mmargaret/Documents/[UVA] Thesis/sherlock-project/log_2/'\n",
    "DIR_OUTPUT = '/Users/mmargaret/Documents/[UVA] Thesis/sherlock-project/output_2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8855b7c",
   "metadata": {},
   "source": [
    "### Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce71eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='{}{}'.format(DIR_LOG,datetime.now().strftime('%Y%m%d_%H%M_sherlock.log')), mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s : %(message)s', datefmt='%m/%d/%Y %I:%M')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.info('- LOGGING STARTS -')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6bdd41",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30fe9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import os\n",
    "import sys\n",
    "    \n",
    "from sherlock import helpers\n",
    "from sherlock.deploy.model import SherlockModel\n",
    "from sherlock.functional import extract_features_to_csv\n",
    "from sherlock.features.paragraph_vectors import initialise_pretrained_model, initialise_nltk\n",
    "from sherlock.features.preprocessing import (\n",
    "    extract_features,\n",
    "    convert_string_lists_to_lists,\n",
    "    prepare_feature_extraction,\n",
    "    load_parquet_values,\n",
    ")\n",
    "from sherlock.features.word_embeddings import initialise_word_embeddings\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1101303",
   "metadata": {},
   "source": [
    "### Initialize Sherlock's feature extraction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8682ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature extraction by downloading 4 files:\n",
      "        \n",
      " ../sherlock/features/glove.6B.50d.txt, \n",
      " ../sherlock/features/par_vec_trained_400.pkl.docvecs.vectors_docs.npy,\n",
      "        \n",
      " ../sherlock/features/par_vec_trained_400.pkl.trainables.syn1neg.npy, and \n",
      " ../sherlock/features/par_vec_trained_400.pkl.wv.vectors.npy.\n",
      "        \n",
      "All files for extracting word and paragraph embeddings are present.\n",
      "Initialising word embeddings\n",
      "Initialise Word Embeddings process took 0:00:04.550186 seconds.\n",
      "Initialise Doc2Vec Model, 400 dim, process took 0:00:02.432000 seconds. (filename = ../sherlock/features/par_vec_trained_400.pkl)\n",
      "Initialised NLTK, process took 0:00:00.188585 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mmargaret/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mmargaret/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "prepare_feature_extraction()\n",
    "initialise_word_embeddings()\n",
    "initialise_pretrained_model(400)\n",
    "initialise_nltk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f30d7",
   "metadata": {},
   "source": [
    "### Get the list of all data files for processing\n",
    "Retrieve all CSV filenames in the specified folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba26aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = os.listdir(DIR_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bacb96ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Files: 10\n"
     ]
    }
   ],
   "source": [
    "file_list = [id for id in _ if '.csv' in id]\n",
    "logging.info('Number of Files: {}'.format(len(file_list)))\n",
    "print('Number of Files: {}'.format(len(file_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6095f7",
   "metadata": {},
   "source": [
    "## 2. Define Utilities Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1736dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictedLabels(y_pred_proba, classes, threshold=0.0):\n",
    "    \"\"\"\n",
    "        This function retrieve the predicted semantics by assigning classes with highest probability \n",
    "        that is at least the same or higher than the parameter: \"thresholds\". \n",
    "        Input: \n",
    "            y_pred_proba: \n",
    "            classes:\n",
    "            threshold:\n",
    "        Output: returns a list of predicted semantics \n",
    "    \"\"\"\n",
    "    pred_scores = np.max(y_pred_proba, axis=1)\n",
    "    index_threshold = np.where(pred_scores >= threshold)[0]\n",
    "    y_pred_int = np.argmax(y_pred_proba, axis=1)[index_threshold]\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = classes\n",
    "\n",
    "    return encoder.inverse_transform(y_pred_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5379cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgColumnLength(df):\n",
    "    \"\"\"\n",
    "        This function calculates the average length of each columns in the dataframe.\n",
    "    \"\"\"\n",
    "    len_list = []\n",
    "    for col in df:\n",
    "        len_list += [round(df[col].apply(len).mean(),4)]\n",
    "    return len_list\n",
    "\n",
    "def pct_completeness(df):\n",
    "    \"\"\"\n",
    "        This function calculates the percentage of completeness of each columns in the dataframe.\n",
    "    \"\"\"\n",
    "    pct_list = []\n",
    "    df_len = len(df)\n",
    "    for col in df:\n",
    "        pct_list += [round(1 - df[col].isna().sum() / df_len,4)]\n",
    "    return pct_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac876f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractIDSemanticsWithColumnNames(filename):\n",
    "    \"\"\"\n",
    "        This function:\n",
    "        (1) read the dataset given by the \"filename\", \n",
    "        (2) using Sherlock to: extract their features, initialise Sherlock model and predict their semantics. \n",
    "        (3) extract other features, such as column names and column types\n",
    "        Input: \n",
    "            filename: \n",
    "        Output: returns a dictionary containing semantics and features of one data file\n",
    "    \"\"\"\n",
    "    \n",
    "    IDSemanticsColumns = {'data_filename':filename, 'colSemantics': [], 'colNames':[]}\n",
    "    try:\n",
    "        # read files\n",
    "        with open(DIR_DATASET + filename, errors='ignore') as f:\n",
    "            a_doc = pd.read_csv(f)\n",
    "        \n",
    "        # column stats\n",
    "        col_types = a_doc.dtypes.tolist()\n",
    "        col_complete = pct_completeness(a_doc)\n",
    "        \n",
    "        a_doc = a_doc.select_dtypes(include=[object]).astype(str)\n",
    "        col_len = avgColumnLength(a_doc)\n",
    "        data = pd.Series(a_doc.transpose().values.tolist(), name=\"values\") #format it to list of values by columns\n",
    "\n",
    "        # sherlock extract features\n",
    "        extract_features(\"../temporary.csv\",data)\n",
    "        feature_vectors = pd.read_csv(\"../temporary.csv\", dtype=np.float32)\n",
    "\n",
    "        # sherlock init and predict with pre-trained model\n",
    "        model = SherlockModel();\n",
    "        model.initialize_model_from_json(with_weights=True, model_id=model_id);\n",
    "        \n",
    "        # PREDICT\n",
    "        predicted_proba = model.predict_proba(feature_vectors, model_id)\n",
    "        predicted_scores = np.max(predicted_proba, axis=1).round(4)\n",
    "\n",
    "        # return dictionary with id: id of the doc, list of the columns' semantics, list of the columns' names\n",
    "        IDSemanticsColumns = {'data_filename':filename\n",
    "                              , 'colSemantics': list(getPredictedLabels(predicted_proba, classes))\n",
    "                              , 'colSemantics_s10': list(getPredictedLabels(predicted_proba, classes, 0.1))\n",
    "                              , 'colSemantics_s20': list(getPredictedLabels(predicted_proba, classes, 0.2))\n",
    "                              , 'colSemantics_s30': list(getPredictedLabels(predicted_proba, classes, 0.3))\n",
    "                              , 'colSemantics_s40': list(getPredictedLabels(predicted_proba, classes, 0.4))\n",
    "                              , 'colSemantics_s50': list(getPredictedLabels(predicted_proba, classes, 0.5))\n",
    "                              , 'colSemantics_s60': list(getPredictedLabels(predicted_proba, classes, 0.6))\n",
    "                              , 'colSemantics_s70': list(getPredictedLabels(predicted_proba, classes, 0.7))\n",
    "                              , 'colSemantics_s80': list(getPredictedLabels(predicted_proba, classes, 0.8))\n",
    "                              , 'colSemantics_s90': list(getPredictedLabels(predicted_proba, classes, 0.9))\n",
    "                              , 'colSemantics_s95': list(getPredictedLabels(predicted_proba, classes, 0.95))\n",
    "                              , 'colSemantics_s98': list(getPredictedLabels(predicted_proba, classes, 0.98))\n",
    "                              , 'colSemantics_s99': list(getPredictedLabels(predicted_proba, classes, 0.99))\n",
    "                              , 'colNames':list(a_doc.columns)\n",
    "                              , 'colTypes': col_types\n",
    "                              , 'colLen': col_len\n",
    "                              , 'colComplete': col_complete\n",
    "                              , 'colScores': list(predicted_scores)}\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error('Unable to extract: {}'.format(filename))\n",
    "        print('Unable to extract: {}'.format(filename))\n",
    "        \n",
    "        print(e)\n",
    "        logging.error(e, exc_info=True)\n",
    "        \n",
    "        global error_list\n",
    "        error_list += [filename]\n",
    "        \n",
    "    return IDSemanticsColumns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd5b3d",
   "metadata": {},
   "source": [
    "### Test Extraction of Semantics\n",
    "Test for extraction of one sample of data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a60116cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "error_list=[] # initialise the list to store filenames with prediction errors\n",
    "model_id = \"sherlock\"\n",
    "classes = np.load(f\"../model_files/classes_{model_id}.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bccb8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Extracting Features:   0%|                                | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8238e6a8bbb8896f3d7e346013e6356cd6101aa03236beb1f8bbbc1326dd51a5.text.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Extracting Features:  12%|███                     | 1/8 [00:00<00:01,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 8/8 [00:00<00:00,  8.01it/s]\n",
      "2022-07-14 23:35:01.167279: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-07-14 23:35:01.183568: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f97cf8f83f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-07-14 23:35:01.183582: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_filename': '8238e6a8bbb8896f3d7e346013e6356cd6101aa03236beb1f8bbbc1326dd51a5.text.csv', 'colSemantics': ['company', 'location', 'address', 'city', 'state', 'category', 'duration', 'status'], 'colSemantics_s10': ['company', 'location', 'address', 'city', 'state', 'category', 'duration', 'status'], 'colSemantics_s20': ['company', 'location', 'address', 'city', 'state', 'category', 'duration', 'status'], 'colSemantics_s30': ['company', 'address', 'city', 'state', 'category', 'duration', 'status'], 'colSemantics_s40': ['company', 'address', 'city', 'state', 'category', 'duration', 'status'], 'colSemantics_s50': ['company', 'address', 'city', 'state', 'category', 'duration'], 'colSemantics_s60': ['company', 'address', 'city', 'state', 'duration'], 'colSemantics_s70': ['company', 'address', 'city', 'state', 'duration'], 'colSemantics_s80': ['company', 'address', 'city', 'state'], 'colSemantics_s90': ['company', 'address', 'city', 'state'], 'colSemantics_s95': ['company', 'address', 'city', 'state'], 'colSemantics_s98': ['company', 'address', 'city', 'state'], 'colSemantics_s99': ['company', 'address', 'city', 'state'], 'colNames': ['Facility Name', 'Alternate CCN', 'Address', 'City', 'State', 'Measure Name', 'Pain Assessment and Follow-up Measure Score', 'Pain Assessment and Follow-up Reason for No Score (See Footnotes File)'], 'colTypes': [dtype('O'), dtype('int64'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('int64'), dtype('int64'), dtype('O'), dtype('O'), dtype('O'), dtype('int64'), dtype('int64')], 'colLen': [26.7952, 1.2018, 22.4428, 8.829, 2.0, 15.0, 2.2508, 1.0491], 'colComplete': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'colScores': [0.9906, 0.2577, 1.0, 1.0, 0.9999, 0.553, 0.7988, 0.4201]}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# TEST function\n",
    "logging.info('- TEST START -')\n",
    "\n",
    "test_file = file_list[1]\n",
    "print(test_file)\n",
    "logging.info('filename: {}'.format(test_file))\n",
    "\n",
    "test_extract = extractIDSemanticsWithColumnNames(test_file)\n",
    "print (test_extract)\n",
    "logging.info('extraction: {}'.format(test_extract))\n",
    "\n",
    "print (error_list)\n",
    "logging.info('error list: {}'.format(error_list))\n",
    "\n",
    "logging.info('- TEST END -')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0b1b6",
   "metadata": {},
   "source": [
    "## 3. Semantics Extraction\n",
    "begin to predict semantics for all data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3001d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = os.listdir(DIR_OUTPUT) # retrieve filenamne\n",
    "enrich_list = [] # initiliase the list to store the output of \n",
    "output_filenames = [] # initialise the list to store filenames that have been processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec9ae30",
   "metadata": {},
   "source": [
    "### Get the list of latest extracted semantics\n",
    "If you are running a huge volume of files and require to continue from the latest run, use it. \n",
    "If you want to run a fresh run each time, skip this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "defa3fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Output Files: 292\n",
      "Latest Output Filename: /Users/mmargaret/Documents/[UVA] Thesis/sherlock-project/output_2/enriched_part_990.csv\n",
      "Number of Extracted Dataset: 991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['7cdef5be079976d589563498ba1801d9317588a120c4816961c08dbe696b6af1.text.csv',\n",
       " '2dfdae56b7e139b560c7aae93f0845bb1baf19c5a2aaa9fd56b111838808365d.text.csv',\n",
       " 'f005d30ac771e9601534e78e88d1787f6021f2392979a5a6834d8e58e1ebcab2.text.csv',\n",
       " 'd97dca84965ad6da866ce3c590afe9723e8d885b9f5c7ce2e2a7d640d667eb4e.text.csv',\n",
       " '74a0b0f764269f666108703102cd2556cf9bcb9399b788ab96a49e6aa234f6c5.text.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    output_list = [DIR_OUTPUT + str(id) for id in _ if 'enriched_part_' in id]\n",
    "    logging.info('Number of Output Files: {}'.format(len(output_list)))\n",
    "    print('Number of Output Files: {}'.format(len(output_list)))\n",
    "\n",
    "    latest_output = max(output_list, key=os.path.getctime)\n",
    "    logging.info('Latest Output Filename: {}'.format(latest_output))\n",
    "    print('Latest Output Filename: {}'.format(latest_output))\n",
    "\n",
    "    output_df = pd.read_csv(latest_output)\n",
    "    output_filenames = output_df['data_filename'].tolist()\n",
    "    enrich_list = output_df.to_dict('records')\n",
    "    \n",
    "    logging.info('Number of Extracted Dataset: {}'.format(len(output_filenames)))\n",
    "    print('Number of Extracted Dataset: {}'.format(len(output_filenames)))\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error('Unable to retrieve latest output')\n",
    "    print('Unable to retrieve latest output')\n",
    "    \n",
    "    logging.error(e, exc_info=True)\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "output_filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12701567",
   "metadata": {},
   "source": [
    "### Start Extraction \n",
    "Predict semantics for all data files in the specified folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "698ce676",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('- EXTRACT START -')\n",
    "\n",
    "error_list=[] # reset the list to keep track of filenames with prediction that were not successful\n",
    "model_id = \"sherlock\"\n",
    "classes = np.load(f\"../model_files/classes_{model_id}.npy\", allow_pickle=True)\n",
    "col_csv = ['data_filename', 'colSemantics'\n",
    "           , 'colSemantics_s10', 'colSemantics_s20', 'colSemantics_s30', 'colSemantics_s40', 'colSemantics_s50'\n",
    "           , 'colSemantics_s60', 'colSemantics_s70', 'colSemantics_s80', 'colSemantics_s90', 'colSemantics_s95'\n",
    "           , 'colSemantics_s98', 'colSemantics_s99', 'colNames', 'colScores', 'colComplete', 'colTypes', 'colLen']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d22ff0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   9%|██                     | 1/11 [00:00<00:01,  8.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 11/11 [00:02<00:00,  5.28it/s]\n",
      "/Users/mmargaret/opt/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  if sys.path[0] == '':\n",
      "Extracting Features:  50%|████████████            | 1/2 [00:00<00:00,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- i: 0 -\n",
      "Existed: 8238e6a8bbb8896f3d7e346013e6356cd6101aa03236beb1f8bbbc1326dd51a5.text.csv skipped\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 2/2 [00:00<00:00,  6.42it/s]\n",
      "Extracting Features:  45%|██████████▍            | 5/11 [00:00<00:00, 48.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed: 7c9bbf21459a35f1026578ed83c0133c542f86ad6ca7b3b68976fc446e83e20e.text.csv skipped\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 11/11 [00:00<00:00, 11.75it/s]\n",
      "Extracting Features:   0%|                               | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████████████████| 23/23 [00:03<00:00,  7.34it/s]\n",
      "Extracting Features:   0%|                                | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed: a0a9481da41ddd2fd7ff50e960f30b3e2683e66ee1e15861de5ef76f0322179a.text.csv skipped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Extracting Features:  12%|███                     | 1/8 [00:00<00:01,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 8/8 [00:00<00:00,  8.12it/s]\n",
      "Extracting Features:  50%|████████████            | 1/2 [00:00<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed: b0bbd0acf11ea708872f5275d1e73fca95cd25fe53e3457ea972d7ed49357a7c.text.csv skipped\n",
      "Exporting 1588 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|████████████████████████| 2/2 [00:00<00:00,  6.27it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each data file: \n",
    "- if datafile had been previously processed, skip the file. \n",
    "- otherwise, extract semantics prediction, column names, etc; store them in a list.\n",
    "- for every 10 data files processed, store a temporary file containing latest collection of prediction and error.\n",
    "\"\"\"\n",
    "\n",
    "for i in range(0, len(file_list)):\n",
    "    \n",
    "    # so that it does not need to rerun existing output\n",
    "    if (file_list[i] in output_filenames):\n",
    "        logging.info('Existed: {} skipped'.format(file_list[i]))\n",
    "        print('Existed: {} skipped'.format(file_list[i]))\n",
    "        continue\n",
    "        \n",
    "    enrich_list += [extractIDSemanticsWithColumnNames(file_list[i])]\n",
    "    if i%10==0:\n",
    "        \n",
    "        logging.info('i: {}'.format(i))\n",
    "        sys.stdout.write('- i: {} -'.format(i))\n",
    "        sys.stdout.write('\\n')\n",
    "        \n",
    "        pd.DataFrame(enrich_list\n",
    "                     , columns = col_csv).to_csv(DIR_OUTPUT +'enriched_part_' + str(i) +'.csv'\n",
    "                     , index=False)\n",
    "        \n",
    "        pd.DataFrame(error_list\n",
    "             , columns=['data_filename']).to_csv(DIR_OUTPUT + 'error_part_' + str(i) +'.csv'\n",
    "             , index=False)\n",
    "        \n",
    "logging.info('- EXTRACT END -')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f65cf9c",
   "metadata": {},
   "source": [
    "## 4. Export \n",
    "Export extracted semantics and list of files with errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad6a1153",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(enrich_list\n",
    "             , columns = col_csv).to_csv(DIR_OUTPUT +'enriched_all.csv'\n",
    "             , index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00760329",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(error_list\n",
    "             , columns=['data_filename']).to_csv(DIR_OUTPUT + 'error_all.csv'\n",
    "             , index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80fdfcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('- EOF -')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
